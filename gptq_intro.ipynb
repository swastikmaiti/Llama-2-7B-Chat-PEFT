{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:51:43.828356Z","iopub.status.busy":"2024-04-27T10:51:43.827671Z","iopub.status.idle":"2024-04-27T10:52:12.746194Z","shell.execute_reply":"2024-04-27T10:52:12.745240Z","shell.execute_reply.started":"2024-04-27T10:51:43.828321Z"},"trusted":true},"outputs":[],"source":["!pip install optimum\n","!pip install auto-gptq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:52:12.748474Z","iopub.status.busy":"2024-04-27T10:52:12.748203Z","iopub.status.idle":"2024-04-27T10:52:17.238206Z","shell.execute_reply":"2024-04-27T10:52:17.237205Z","shell.execute_reply.started":"2024-04-27T10:52:12.748448Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import nltk\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"markdown","metadata":{},"source":["# Qunatization: Compressing LLM"]},{"cell_type":"markdown","metadata":{},"source":["### Comparing in-memory size of Normal model with Quantized Model\n","#### We will be working with facebook/opt-125m"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:52:46.437404Z","iopub.status.busy":"2024-04-27T10:52:46.437108Z","iopub.status.idle":"2024-04-27T10:52:59.996214Z","shell.execute_reply":"2024-04-27T10:52:59.995373Z","shell.execute_reply.started":"2024-04-27T10:52:46.437382Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","model_checkpoint= \"facebook/opt-125m\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:52:59.997900Z","iopub.status.busy":"2024-04-27T10:52:59.997392Z","iopub.status.idle":"2024-04-27T10:53:00.003600Z","shell.execute_reply":"2024-04-27T10:53:00.002644Z","shell.execute_reply.started":"2024-04-27T10:52:59.997872Z"},"trusted":true},"outputs":[],"source":["unquantized_memory_footprint = model.get_memory_footprint()/1024**2"]},{"cell_type":"markdown","metadata":{},"source":["#### For parameters details on GPTQConfig check [@HuggingFace Documentation](https://huggingface.co/docs/transformers/v4.33.0/en/main_classes/quantization)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:53:00.005183Z","iopub.status.busy":"2024-04-27T10:53:00.004909Z","iopub.status.idle":"2024-04-27T10:56:45.848038Z","shell.execute_reply":"2024-04-27T10:56:45.847238Z","shell.execute_reply.started":"2024-04-27T10:53:00.005160Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n","model_id = \"facebook/opt-125m\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n","\n","quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:45.849988Z","iopub.status.busy":"2024-04-27T10:56:45.849347Z","iopub.status.idle":"2024-04-27T10:56:45.856136Z","shell.execute_reply":"2024-04-27T10:56:45.855264Z","shell.execute_reply.started":"2024-04-27T10:56:45.849961Z"},"trusted":true},"outputs":[],"source":["quantized_memory_footprint = quantized_model.get_memory_footprint()/1024**2"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:45.857513Z","iopub.status.busy":"2024-04-27T10:56:45.857252Z","iopub.status.idle":"2024-04-27T10:56:45.870059Z","shell.execute_reply":"2024-04-27T10:56:45.869207Z","shell.execute_reply.started":"2024-04-27T10:56:45.857489Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Model Size:477.75 MB, Quantized Model Size:119.2734375 MB Difference:358.4765625 MB\n"]}],"source":["print(f'Original Model Size:{unquantized_memory_footprint} MB, Quantized Model Size:{quantized_memory_footprint} MB Difference:{unquantized_memory_footprint-quantized_memory_footprint} MB')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Check the Quantized Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:45.873852Z","iopub.status.busy":"2024-04-27T10:56:45.873533Z","iopub.status.idle":"2024-04-27T10:56:45.933523Z","shell.execute_reply":"2024-04-27T10:56:45.932780Z","shell.execute_reply.started":"2024-04-27T10:56:45.873828Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'training': True,\n"," '_parameters': OrderedDict(),\n"," '_buffers': OrderedDict([('qweight',\n","               tensor([[ 1711760090, -1248295259, -2025411892,  ..., -1486452502,\n","                         2019142072, -1735820810],\n","                       [-2000132747,  -578262345,  1484081337,  ..., -1230600537,\n","                        -2019252040, -2023311003],\n","                       [ -710293851, -1153090188,  1431922298,  ..., -1768449094,\n","                         2042194587, -2004125258],\n","                       ...,\n","                       [-1183500136, -1494510422, -1772782904,  ..., -1518753378,\n","                         -411710600,  -392845654],\n","                       [-1990626701,  1469278281,  1469864108,  ...,  1740208533,\n","                        -1732560507, -1738077576],\n","                       [ 2015914598,  2040232821,  2005572185,  ..., -1463179655,\n","                        -1450400136, -2024523156]], device='cuda:0', dtype=torch.int32)),\n","              ('qzeros',\n","               tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n","                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n","                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n","                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n","                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n","                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n","                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n","                      device='cuda:0', dtype=torch.int32)),\n","              ('scales',\n","               tensor([[0.0472, 0.0420, 0.0370,  ..., 0.0180, 0.0131, 0.0191],\n","                       [0.0506, 0.0483, 0.0370,  ..., 0.0103, 0.0141, 0.0115],\n","                       [0.0418, 0.0524, 0.0418,  ..., 0.0190, 0.0142, 0.0217],\n","                       [0.0481, 0.0362, 0.0438,  ..., 0.0124, 0.0149, 0.0118],\n","                       [0.0437, 0.0489, 0.0376,  ..., 0.0199, 0.0162, 0.0143],\n","                       [0.0474, 0.0491, 0.0381,  ..., 0.0144, 0.0144, 0.0163]],\n","                      device='cuda:0', dtype=torch.float16)),\n","              ('g_idx',\n","               tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","                       3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n","                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n","                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n","                      device='cuda:0', dtype=torch.int32)),\n","              ('bias',\n","               tensor([-1.4294e-01, -3.6646e-01, -2.9221e-02, -2.8613e-01, -3.0615e-01,\n","                       -2.6880e-01,  7.9773e-02,  8.0261e-02, -1.7957e-01, -5.2887e-02,\n","                        1.4319e-01, -1.8982e-01,  3.6694e-01, -6.1554e-02,  3.2617e-01,\n","                       -2.8857e-01,  6.8481e-02, -2.7271e-01,  1.5649e-01, -2.1838e-01,\n","                       -2.1082e-01, -1.3550e-01, -2.5049e-01, -3.3594e-01,  2.9150e-01,\n","                       -7.9773e-02,  1.3440e-01,  8.8196e-02,  3.8110e-01,  1.4294e-01,\n","                       -7.5989e-02, -1.9873e-01, -2.1973e-02,  1.3397e-02,  1.1505e-01,\n","                        2.5146e-01,  2.8589e-01,  2.9956e-01, -2.2217e-01,  1.3135e-01,\n","                       -2.5269e-01, -2.0190e-01,  8.5083e-02, -5.4901e-02,  2.3267e-01,\n","                       -1.3831e-01, -5.8624e-02, -1.1206e-01,  5.0488e-01,  1.4526e-01,\n","                        5.2295e-01,  2.3157e-01, -1.0779e-01,  2.0374e-01,  1.8323e-01,\n","                       -3.3862e-01,  1.9885e-01, -1.6028e-01, -6.7139e-02,  3.3740e-01,\n","                        1.3025e-01,  1.1078e-01,  3.5034e-01, -2.9590e-01,  5.2539e-01,\n","                        2.8003e-01, -3.3691e-02,  3.4546e-01,  1.3342e-01,  1.0510e-01,\n","                       -1.2433e-01,  2.7686e-01, -4.4098e-02,  6.0921e-03,  1.0352e-01,\n","                        5.0635e-01, -1.7041e-01,  5.0391e-01, -3.5010e-01,  5.4590e-01,\n","                       -1.5039e-01, -1.8030e-01, -1.4600e-01,  3.5498e-01,  1.1182e-01,\n","                        2.5659e-01, -3.5352e-01, -3.8379e-01,  5.3174e-01, -2.0215e-01,\n","                       -2.5928e-01, -1.6821e-01,  3.0304e-02, -2.7026e-01, -4.4312e-01,\n","                       -2.2449e-01,  1.2000e-01,  6.0059e-02,  2.6953e-01,  3.9722e-01,\n","                       -5.0195e-01, -4.9927e-02,  6.1920e-02, -1.1469e-01, -1.8988e-03,\n","                       -6.6650e-02, -1.6272e-01,  3.7134e-01, -3.2568e-01,  7.4196e-03,\n","                        6.5079e-03, -3.2318e-02, -2.0874e-02, -3.4790e-01, -4.3579e-02,\n","                        3.3618e-01, -1.6418e-01, -4.5441e-02,  2.7246e-01,  1.1267e-01,\n","                       -3.1738e-01, -9.1919e-02, -3.3008e-01,  4.5532e-02,  2.4094e-02,\n","                       -2.0227e-01, -1.8384e-01,  9.4910e-02, -5.8167e-02, -2.6343e-01,\n","                       -1.4722e-01,  3.2153e-01, -8.6609e-02, -3.0957e-01,  2.8174e-01,\n","                       -2.3413e-01,  1.7090e-01,  2.5317e-01,  1.2683e-01, -3.0444e-01,\n","                        2.6855e-01,  3.8849e-02,  2.2925e-01,  1.9922e-01,  2.4719e-01,\n","                        4.0942e-01,  1.1780e-01,  1.1475e-01, -6.1127e-02,  2.1313e-01,\n","                        3.8184e-01, -5.8899e-02,  1.0278e-01,  2.3950e-01,  2.8784e-01,\n","                       -3.0457e-02, -1.3049e-01, -3.7646e-01,  2.2119e-01, -1.6406e-01,\n","                       -1.9495e-01,  1.7273e-01, -2.5464e-01,  3.6768e-01, -2.9102e-01,\n","                       -1.5918e-01,  2.0471e-01, -3.2812e-01,  2.2131e-01, -1.1847e-01,\n","                        2.0642e-01,  3.2324e-01, -4.1895e-01, -1.8042e-01, -2.6733e-01,\n","                       -2.9834e-01, -2.4695e-01, -2.6465e-01,  2.0676e-02,  6.9763e-02,\n","                        1.1255e-01,  3.0957e-01,  5.0293e-01,  3.1421e-01,  1.4185e-01,\n","                       -2.6562e-01, -2.4817e-01, -5.6854e-02,  1.8518e-01, -2.5269e-01,\n","                       -2.2961e-01, -8.8867e-02, -4.0796e-01, -1.9324e-01,  4.3262e-01,\n","                       -3.6523e-01,  4.0016e-03,  1.8518e-01,  2.6416e-01,  2.4414e-01,\n","                        9.4177e-02, -1.8005e-01, -1.4246e-01, -2.9395e-01,  3.0396e-01,\n","                       -3.4131e-01, -4.0039e-01, -2.5732e-01, -3.2959e-01,  1.6553e-01,\n","                        2.8491e-01, -2.8394e-01,  5.0342e-01,  2.0190e-01, -4.0576e-01,\n","                        1.8066e-01,  2.8882e-01,  4.3945e-01,  7.6561e-03, -3.0322e-01,\n","                        2.8101e-01,  1.8518e-01, -3.6682e-02,  1.5088e-01,  2.3340e-01,\n","                       -4.3042e-01, -3.6475e-01, -3.6640e-03, -3.4912e-01, -3.8013e-01,\n","                       -4.0436e-02,  3.5474e-01, -4.4434e-01,  5.0830e-01,  3.2373e-01,\n","                        2.6807e-01,  2.6416e-01, -4.2505e-01,  2.9956e-01,  2.4902e-01,\n","                        2.5269e-01,  3.7646e-01, -4.3091e-01,  3.6304e-01,  1.9238e-01,\n","                        1.1406e-02, -9.9258e-03, -2.1826e-01, -4.3396e-02, -3.0566e-01,\n","                       -3.7842e-01, -1.7554e-01, -5.0293e-01, -3.7109e-01,  3.6548e-01,\n","                       -4.4824e-01,  3.2654e-02, -1.1391e-02,  6.0730e-02,  1.7426e-02,\n","                       -1.6431e-01, -8.7219e-02,  1.8884e-01,  7.8552e-02, -4.7821e-02,\n","                        9.8724e-03, -1.9165e-02, -9.9945e-03, -1.0797e-01, -4.2084e-02,\n","                        3.9635e-03, -2.5732e-01,  1.3252e-02,  7.1289e-02,  1.6113e-02,\n","                       -1.3382e-02, -2.1393e-02,  8.0872e-02, -2.5952e-01,  2.6932e-02,\n","                       -1.4763e-02, -2.5284e-02, -2.7905e-01, -3.7659e-02, -1.9409e-02,\n","                       -8.9050e-02, -9.7733e-03, -6.6223e-02,  3.5400e-02, -7.4654e-03,\n","                        6.4697e-02,  1.6052e-01, -1.9073e-02, -1.9806e-02,  3.6835e-02,\n","                        6.1737e-02, -1.0675e-01,  1.5793e-02,  1.3147e-01,  2.5772e-02,\n","                        3.2501e-02,  3.2440e-02, -4.0619e-02,  4.0359e-03, -5.0140e-02,\n","                       -7.7087e-02,  1.2772e-02,  7.0862e-02, -5.2246e-02,  5.9509e-04,\n","                       -2.0569e-02, -4.2999e-02,  1.4465e-02, -2.6562e-01,  5.4359e-03,\n","                       -9.1858e-02, -6.3171e-02,  1.6586e-02,  4.0649e-02,  8.2214e-02,\n","                        2.0844e-02,  5.9967e-03,  4.3488e-02,  2.6047e-02, -2.3384e-03,\n","                       -1.2207e-02, -2.7603e-02,  5.6250e-01, -1.6510e-02, -1.4435e-02,\n","                        5.2930e-01,  2.1805e-02, -1.0262e-03, -2.6810e-02, -2.4414e-03,\n","                       -2.5978e-03,  5.5615e-01, -1.9653e-02, -3.8357e-03, -1.4435e-02,\n","                       -1.5656e-02, -5.8441e-02, -1.7746e-02, -2.8870e-02,  4.0314e-02,\n","                        7.3671e-04,  2.7588e-02, -2.4231e-02,  1.5434e-02,  7.2289e-03,\n","                       -1.5221e-02,  3.0075e-02,  1.4343e-02,  1.7609e-02, -7.1869e-03,\n","                       -2.0630e-02, -3.5736e-02,  3.8574e-02,  6.7139e-04,  1.4320e-02,\n","                        4.2343e-03, -3.5156e-02,  2.5558e-02,  4.6659e-04, -1.2123e-02,\n","                       -2.3880e-02,  1.8906e-02,  7.2937e-03, -2.8259e-02,  1.5465e-02,\n","                       -5.1074e-01,  3.4149e-02,  1.1234e-03,  1.4786e-02,  1.7639e-02,\n","                        5.1849e-02, -6.3591e-03,  4.0741e-03, -2.8381e-03, -2.1534e-03,\n","                       -3.2532e-02,  4.7424e-02,  1.7365e-02,  5.9853e-03,  1.7651e-01,\n","                       -6.3477e-02, -3.6133e-02, -1.9006e-01,  2.5586e-01, -3.6938e-01,\n","                       -3.5034e-01, -1.9250e-01, -3.6938e-01, -1.6724e-01,  3.4619e-01,\n","                        2.4377e-01,  1.3721e-01,  1.8158e-02,  1.2109e-01,  3.1128e-01,\n","                       -4.5228e-04,  1.8616e-01,  4.4580e-01,  2.2864e-01, -3.3661e-02,\n","                       -2.1655e-01,  4.4507e-01, -1.2964e-01, -1.9812e-01, -2.3779e-01,\n","                       -2.4524e-01,  2.4585e-01, -2.5439e-01,  4.7095e-01, -6.5308e-02,\n","                       -3.3765e-01, -4.6045e-01, -1.1108e-01,  2.8442e-01,  1.3000e-01,\n","                        5.0342e-01,  1.7236e-01,  1.1676e-01,  1.6870e-01,  3.2983e-01,\n","                       -2.1204e-01,  1.1945e-01,  9.6313e-02,  1.2390e-01, -3.0811e-01,\n","                        3.0981e-01, -7.5623e-02,  1.7664e-01,  3.6572e-01,  4.6582e-01,\n","                        1.7114e-01, -2.4744e-01,  7.0496e-02, -3.2837e-02,  3.2568e-01,\n","                        1.4648e-01, -2.9785e-01, -1.1273e-01,  1.7529e-01, -3.9429e-01,\n","                       -1.6736e-01, -3.0609e-02, -4.4824e-01, -6.6284e-02,  1.2543e-02,\n","                       -1.6614e-01,  2.9614e-01, -3.0859e-01, -1.7639e-01,  8.0322e-02,\n","                       -3.1348e-01, -2.6514e-01, -2.7295e-01, -2.3755e-01,  1.0217e-01,\n","                        8.8379e-02,  8.1177e-02,  3.8391e-02, -5.9509e-02,  3.3325e-01,\n","                       -7.1594e-02, -7.3669e-02,  2.3544e-02,  4.9347e-02,  1.7932e-01,\n","                        1.3123e-01,  1.7517e-01, -3.6499e-01,  2.6733e-01, -2.6245e-01,\n","                       -8.0490e-03,  7.1487e-03, -2.1204e-01,  5.1123e-01, -1.5356e-01,\n","                       -4.1382e-01, -1.7810e-01, -6.2866e-02,  1.1420e-01,  8.2520e-02,\n","                       -3.1641e-01, -1.5335e-02, -5.0995e-02, -2.9199e-01, -1.7847e-01,\n","                        4.2145e-02,  2.9663e-01, -7.2144e-02, -1.3220e-01,  3.0322e-01,\n","                       -1.2683e-01, -2.8961e-02,  2.2430e-02,  7.2876e-02,  7.6599e-02,\n","                       -2.7145e-02,  2.5854e-01, -1.2128e-01, -3.8867e-01, -6.0699e-02,\n","                        2.7786e-02, -3.5059e-01,  6.4636e-02,  1.3660e-01,  2.9395e-01,\n","                       -1.2329e-01, -8.0444e-02, -9.2163e-02, -5.5518e-01,  1.5649e-01,\n","                       -5.0732e-01, -5.2539e-01,  5.5127e-01, -2.2064e-02, -6.2158e-01,\n","                        5.0586e-01, -5.2490e-01,  1.0199e-01, -2.4683e-01,  5.1807e-01,\n","                        3.6499e-01, -5.5615e-01,  5.6299e-01, -1.1469e-01, -5.1172e-01,\n","                        1.5137e-01,  5.5811e-01, -5.7861e-01, -5.0928e-01,  3.2617e-01,\n","                        2.3938e-01, -5.7666e-01,  1.6016e-01, -2.5488e-01, -3.4863e-01,\n","                        2.4780e-01,  3.9337e-02, -1.6858e-01, -8.2825e-02, -3.3325e-02,\n","                        5.0146e-01,  2.9688e-01,  1.3110e-01,  5.7812e-01,  2.0154e-01,\n","                        8.7585e-02, -5.1416e-01, -2.4902e-01, -1.6833e-01,  5.3760e-01,\n","                       -5.3857e-01,  5.6915e-02,  5.0195e-01,  1.3464e-01,  5.0586e-01,\n","                        6.7383e-02,  1.7346e-01, -2.5781e-01, -5.3857e-01,  7.5455e-03,\n","                       -5.0781e-01, -3.9404e-01,  3.0176e-01,  5.0977e-01, -1.2421e-01,\n","                       -5.0293e-01, -1.2347e-01,  5.1416e-01, -5.3320e-01, -5.7080e-01,\n","                       -3.4644e-01,  5.0781e-01, -7.8796e-02, -4.2236e-02, -2.6221e-01,\n","                        1.0114e-01,  1.0971e-02, -7.5562e-02, -2.2595e-01, -1.4502e-01,\n","                        3.7671e-01,  3.6938e-01,  1.5576e-01,  5.8632e-03,  1.6602e-01,\n","                        2.8955e-01, -4.7217e-01, -1.1194e-01,  2.9077e-01,  2.6758e-01,\n","                       -6.3293e-02,  1.1340e-01,  5.3174e-01,  4.0210e-01,  1.3965e-01,\n","                        1.1505e-01,  8.7891e-02, -8.8562e-02, -4.9023e-01,  3.9380e-01,\n","                        1.3611e-01,  2.3767e-01,  1.9424e-02, -5.5322e-01,  2.3218e-01,\n","                       -5.2881e-01, -1.0211e-01,  2.5854e-01, -3.2520e-01,  1.6632e-02,\n","                       -2.7979e-01,  2.8857e-01, -7.1289e-02, -5.2979e-01,  2.5195e-01,\n","                       -3.1226e-01, -2.7466e-03,  1.2793e-01,  9.7168e-02,  6.2103e-02,\n","                        9.3567e-02,  2.4023e-01,  4.6069e-01, -5.0926e-03,  2.7271e-01,\n","                        1.3184e-01, -2.3901e-01, -3.5205e-01, -1.8982e-01,  3.2806e-02,\n","                        1.8774e-01, -1.1591e-01, -5.9021e-02,  2.8702e-02, -3.8379e-01,\n","                        1.9470e-01,  2.2986e-01,  1.3220e-01,  1.9446e-01,  3.7292e-02,\n","                        1.5076e-01, -6.5308e-02, -3.5461e-02, -7.5867e-02,  1.3379e-01,\n","                        1.0284e-01, -2.7740e-02, -4.9324e-03,  1.3000e-01, -9.5032e-02,\n","                        2.3041e-02,  4.3884e-02,  1.4294e-01, -5.5054e-02,  6.8909e-02,\n","                        5.5939e-02,  6.2408e-02, -2.1582e-01,  2.5000e-01,  5.3076e-01,\n","                       -2.2791e-01, -2.5049e-01,  1.6138e-01, -9.7229e-02, -2.1667e-01,\n","                       -2.4976e-01, -2.5146e-01, -5.2643e-02, -1.2457e-01, -7.0229e-03,\n","                       -1.5002e-01, -8.4473e-02, -9.4681e-03,  8.1055e-02,  9.6252e-02,\n","                        1.7773e-01, -8.1604e-02,  1.3806e-01, -1.8677e-01,  1.6895e-01,\n","                       -3.7628e-02,  5.9662e-02, -9.2773e-02, -7.1533e-02, -1.9885e-01,\n","                        5.9174e-02,  2.5000e-01, -2.3514e-02,  7.3608e-02,  2.2290e-01,\n","                        6.6772e-02, -1.2500e-01, -1.3550e-01, -6.0028e-02, -1.3147e-01,\n","                       -1.2756e-01,  6.3416e-02,  1.3062e-01,  1.2488e-01,  6.1584e-02,\n","                       -6.7101e-03, -3.3875e-02, -1.6556e-02,  1.8738e-02, -4.7836e-03,\n","                       -3.2776e-02,  1.4427e-02, -5.1221e-01, -4.6661e-02,  3.5343e-03,\n","                        1.5762e-02,  2.3918e-03, -3.1250e-02, -1.8661e-02, -3.0685e-02,\n","                        1.5762e-02, -8.2779e-03, -7.0190e-02,  1.4915e-02, -1.6663e-02,\n","                        2.6138e-02, -8.9645e-03,  1.2150e-03, -1.5222e-01, -2.7023e-02,\n","                       -2.4277e-02, -8.3237e-03,  2.5192e-02, -7.4120e-03, -7.1487e-03,\n","                       -3.6377e-01,  7.0496e-02, -2.0630e-01,  2.1744e-02,  2.2049e-02,\n","                       -1.7487e-02, -5.1910e-02,  3.1799e-02,  1.8585e-02,  2.3880e-02,\n","                       -1.8433e-02,  9.0866e-03,  1.8753e-02,  1.7273e-02, -4.1580e-03,\n","                        1.5976e-02, -2.8519e-02,  4.3030e-03, -2.9312e-02,  3.5217e-02,\n","                        4.0771e-02, -1.8188e-01,  9.0103e-03,  5.0586e-01,  2.0020e-02,\n","                        1.9806e-02,  1.6861e-02,  2.5436e-02,  1.7548e-02, -8.7357e-03,\n","                       -1.6785e-02,  1.8906e-02, -1.3695e-02], device='cuda:0',\n","                      dtype=torch.float16))]),\n"," '_non_persistent_buffers_set': set(),\n"," '_backward_pre_hooks': OrderedDict(),\n"," '_backward_hooks': OrderedDict(),\n"," '_is_full_backward_hook': None,\n"," '_forward_hooks': OrderedDict(),\n"," '_forward_hooks_with_kwargs': OrderedDict(),\n"," '_forward_hooks_always_called': OrderedDict(),\n"," '_forward_pre_hooks': OrderedDict(),\n"," '_forward_pre_hooks_with_kwargs': OrderedDict(),\n"," '_state_dict_hooks': OrderedDict(),\n"," '_state_dict_pre_hooks': OrderedDict(),\n"," '_load_state_dict_pre_hooks': OrderedDict(),\n"," '_load_state_dict_post_hooks': OrderedDict(),\n"," '_modules': OrderedDict(),\n"," 'infeatures': 768,\n"," 'outfeatures': 768,\n"," 'bits': 4,\n"," 'group_size': 128,\n"," 'maxq': 15,\n"," 'half_indim': 384,\n"," 'use_cuda_fp16': True,\n"," 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n"," 'kernel_switch_threshold': 128,\n"," 'autogptq_cuda_available': False,\n"," 'autogptq_cuda': None,\n"," 'trainable': False,\n"," 'device': device(type='cuda', index=0)}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["quantized_model.model.decoder.layers[0].self_attn.q_proj.__dict__ ## contain qweights and qzeors in int32 format whereas normal model has ony weights and bias in FP16/FP32 format"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Comparing outputs"]},{"cell_type":"markdown","metadata":{},"source":["#### Quantized Model output"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:45.934863Z","iopub.status.busy":"2024-04-27T10:56:45.934578Z","iopub.status.idle":"2024-04-27T10:56:49.441512Z","shell.execute_reply":"2024-04-27T10:56:49.440513Z","shell.execute_reply.started":"2024-04-27T10:56:45.934840Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ML is different from classical algorithm in the fact that it is a very complex algorithm.                                                                                  \n"]}],"source":["text = \"ML is different from classical algorithm in the fact that\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","out = quantized_model.generate(**inputs,max_length=100,min_new_tokens=20)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["#### Normal model output"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:49.443233Z","iopub.status.busy":"2024-04-27T10:56:49.442845Z","iopub.status.idle":"2024-04-27T10:56:50.275503Z","shell.execute_reply":"2024-04-27T10:56:50.274356Z","shell.execute_reply.started":"2024-04-27T10:56:49.443197Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ML is different from classical algorithm in the fact that it is not a linear algorithm.\n","I'm not sure what you mean by linear.\n","It's a linear algorithm that is not linear in that it is not a linear algorithm.\n","I'm not sure what you mean by linear.\n","It's a linear algorithm that is not linear in that it is not a linear algorithm.\n","I'm not sure what you mean by linear.\n","It's a linear algorithm that is not linear in\n"]}],"source":["text = \"ML is different from classical algorithm in the fact that\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","out = model.generate(**inputs,max_length=100,min_new_tokens=20)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["### Quantizing a model with custom dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:56:50.277261Z","iopub.status.busy":"2024-04-27T10:56:50.276883Z","iopub.status.idle":"2024-04-27T10:57:34.086517Z","shell.execute_reply":"2024-04-27T10:57:34.085518Z","shell.execute_reply.started":"2024-04-27T10:56:50.277226Z"},"trusted":true},"outputs":[],"source":["quantization_config = GPTQConfig(\n","    bits=4,\n","    group_size=128,\n","    desc_act=False,\n","    dataset=[\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-27T10:57:34.088289Z","iopub.status.busy":"2024-04-27T10:57:34.087986Z","iopub.status.idle":"2024-04-27T10:57:36.429007Z","shell.execute_reply":"2024-04-27T10:57:36.428025Z","shell.execute_reply.started":"2024-04-27T10:57:34.088264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ML is different from classical algorithm in the fact that.                                                                                        \n"]}],"source":["text = \"ML is different from classical algorithm in the fact that\"\n","#text = \"Hello my name is\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","out = quant_model.generate(**inputs,max_length=100,min_new_tokens=20)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
